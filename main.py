# -*- coding: utf-8 -*-
"""
LLM æ¦‚ç‡æ¨æ¼”æ¼”ç¤ºï¼šQwen1.5-1.8B-Chat
-----------------------------------
ç›®æ ‡ï¼šå±•ç¤ºæ¨¡å‹ç”Ÿæˆæ¯ä¸ª token æ—¶çš„ top-10 æ¦‚ç‡åˆ†å¸ƒ
æ­¥éª¤ï¼š
  1. ä½¿ç”¨å›½å†…é•œåƒåŠ è½½æœ€æ–°æ¨¡å‹
  2. é€ token ç”Ÿæˆæ–‡æœ¬
  3. æ‰“å°æ¯ä¸€æ­¥çš„æ¦‚ç‡åˆ†å¸ƒ
  4. å±•ç¤º LLM çš„â€œæ€è€ƒè¿‡ç¨‹â€
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_NAME = "Qwen/Qwen1.5-1.8B-Chat"

print("æ­£åœ¨ä»é•œåƒåŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

print("æ­£åœ¨ä»é•œåƒåŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,  # å¼ºåˆ¶ä½¿ç”¨ float16
    device_map={"": "cpu"},  # ä½¿ç”¨ CPU
    trust_remote_code=True,
    load_in_8bit=False,
    attn_implementation="eager"  # é¿å…ä½¿ç”¨ Flash Attention
)

model.eval()


# ================== 4. æ¦‚ç‡æ¨æ¼”å‡½æ•° ==================
def generate_with_probs(prompt, max_new_tokens=30):
    """
    ç”Ÿæˆæ–‡æœ¬å¹¶æ‰“å°æ¯ä¸€æ­¥ top-10 æ¦‚ç‡åˆ†å¸ƒ
    è¿”å›ï¼šæœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    print(f"\nğŸ“ æç¤ºè¯: {prompt}\n")

    output_content = ""  # å»ºè®®ç”¨ snake_case é£æ ¼

    for i in range(max_new_tokens):
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            next_token_logits = logits[:, -1, :]  # æœ€åä¸€ä¸ª token çš„ logits

            # è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            probs = torch.softmax(next_token_logits, dim=-1)
            sorted_probs, sorted_indices = torch.topk(probs, 10)  # å– top-10

            # è§£ç  top-10 çš„ token
            top_tokens = tokenizer.batch_decode(sorted_indices[0], skip_special_tokens=False)
            top_probs = sorted_probs[0].cpu().numpy()

            # æ‰“å°æ¯ä¸€æ­¥çš„æ¦‚ç‡
            print(f"æ­¥éª¤ {i + 1}:")
            for token, prob in zip(top_tokens, top_probs):
                print(f"  '{token}': {prob:.4f} ({prob * 100:.2f}%)")

            # è´ªå¿ƒç­–ç•¥ï¼šé€‰æ¦‚ç‡æœ€é«˜çš„ token
            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)
            new_text = tokenizer.decode(next_token[0], skip_special_tokens=False)
            print(f"  â¡ï¸  é€‰æ‹©: '{new_text}'\n")

            # âœ… è¿½åŠ åˆ°è¾“å‡ºå†…å®¹
            output_content += new_text

            # æ›´æ–°è¾“å…¥åºåˆ—
            inputs = {
                'input_ids': torch.cat([inputs['input_ids'], next_token], dim=-1),
                'attention_mask': torch.cat([
                    inputs['attention_mask'],
                    torch.ones((1, 1), device=inputs['attention_mask'].device)
                ], dim=-1)
            }

            # å¦‚æœç”Ÿæˆç»“æŸç¬¦ï¼Œæå‰é€€å‡º
            if next_token.item() == tokenizer.eos_token_id:
                print("âœ… é‡åˆ°ç»“æŸç¬¦ï¼Œç”Ÿæˆç»ˆæ­¢ã€‚\n")
                break

    # âœ… æ‰“å°æœ€ç»ˆç»“æœ
    print("âœ¨ æœ€ç»ˆç”Ÿæˆç»“æœ:")
    print(f"ã€Œ{output_content}ã€\n")

    # âœ… è¿”å›ç»“æœï¼Œä¾¿äºåç»­ä½¿ç”¨
    return output_content

# ================== 5. è¿è¡Œæ¨æ¼” ==================
if __name__ == "__main__":
    # prompt = "ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰åŒç†å¿ƒçš„å›ç­”è€…ã€‚è¯·å›ç­”ï¼šçˆ±æƒ…æ˜¯ä»€ä¹ˆï¼Ÿ"
    prompt = "çˆ±æƒ…æ˜¯ï¼Ÿ"
    result = generate_with_probs(prompt, max_new_tokens=30)